{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'C:/Users/jack/Documents/ProductData'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Get the most recent CSV file\n",
    "most_recent_file = max(csv_files, key=os.path.getctime)\n",
    "\n",
    "# Read the data from the most recent CSV file\n",
    "df = pd.read_csv(most_recent_file)\n",
    "\n",
    "# Lowercase the column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Lowercase all entries\n",
    "df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all csv files into one\n",
    "os.chdir(\"C:/Users/jack/Documents/SoldDates\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "\n",
    "#export to csv\n",
    "combined_csv.to_csv(\"C:/Users/jack/Documents/sold_dates_combined.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('C:/Users/jack/Documents/sold_dates_combined.csv')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Align and add combined csv file to the main csv file, by Item Number\n",
    "\n",
    "\n",
    "# Read the data from the most recent CSV file\n",
    "# Set the path to the folder containing the CSV files\n",
    "folder_path = 'C:/Users/jack/Documents/ProductData'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Get the most recent CSV file\n",
    "most_recent_file = 'C:/Users/jack/Documents/ProductData/dell+inspiron+3793+laptop+computer_output.csv'\n",
    "#max(csv_files, key=os.path.getctime)\n",
    "\n",
    "\n",
    "# Read the data from the most recent CSV file\n",
    "df = pd.read_csv(most_recent_file)\n",
    "\n",
    "# Lowercase the column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Lowercase all entries\n",
    "df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "df2 = pd.read_csv('C:/Users/jack/Documents/sold_dates_combined.csv')\n",
    "\n",
    "#  Rename item_number column to Item Number\n",
    "df2.rename(columns={'item_number': 'item number'}, inplace=True)\n",
    "\n",
    "\n",
    "# Merge the dataframes on the 'item number' column\n",
    "df3 = pd.merge(df, df2, on='item number', how='left')\n",
    "\n",
    "\n",
    "\n",
    "#df3 = df.merge(df2, on='item number', how='left')\n",
    "display(df3)\n",
    "\n",
    "# Write the DataFrame to a CSV file and display it\n",
    "df3.to_csv(f'C:/Users/jack/Documents/AlignedComplete.csv', index=False)\n",
    "\n",
    "# Check for duplicate rows in the merged dataframe\n",
    "duplicates = df3[df3.duplicated()]\n",
    "\n",
    "# Print the number of duplicate rows\n",
    "print(f'The merged dataframe contains {len(duplicates)} duplicate rows.')\n",
    "\n",
    "# Remove duplicate rows from the merged dataframe\n",
    "df3.drop_duplicates(inplace=True)\n",
    "\n",
    "# Print the number of rows in the cleaned dataframe\n",
    "print(f'The cleaned dataframe contains {len(df3)} rows.')\n",
    "\n",
    "# Check for missing values in the merged dataframe\n",
    "missing_values = df3.isnull().sum()\n",
    "\n",
    "# Print the number of missing values for each column\n",
    "print(missing_values)\n",
    "\n",
    "# Write the DataFrame to a CSV file and display it\n",
    "df3.to_csv(f'C:/Users/jack/Documents/AlignedComplete.csv', index=False)\n",
    "\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows which do not contain a price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('C:/Users/jack/Documents/AlignedComplete.csv')\n",
    "\n",
    "# Drop duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop rows with missing 'Price' values\n",
    "df.dropna(subset=['price'], inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove job lot listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with 'Job lot' in the 'Title', 'Seller notes', or 'Condition' column\n",
    "df = df[~df['title'].str.contains('job lot')]\n",
    "df['seller notes'] = df['seller notes'].fillna('')\n",
    "df = df[~df['seller notes'].str.contains('job lot')]\n",
    "df['condition'] = df['condition'].astype(str) # Convert to string\n",
    "df = df[~df['condition'].str.contains('job lot')]\n",
    "\n",
    "# Drop rows with 'faulty' in the 'Title', 'Seller notes', or 'Condition' column\n",
    "df = df[~df['title'].str.contains('faulty')]\n",
    "df = df[~df['seller notes'].str.contains('faulty')]\n",
    "df['condition'] = df['condition'].astype(str) # Convert to string\n",
    "df = df[~df['condition'].str.contains('faulty')]\n",
    "\n",
    "# Drop rows with 'spares' in the 'Title', 'Seller notes', or 'Condition' column\n",
    "df = df[~df['title'].str.contains('spares')]\n",
    "df = df[~df['seller notes'].str.contains('spares')]\n",
    "df['condition'] = df['condition'].astype(str) # Convert to string\n",
    "df = df[~df['condition'].str.contains('spares')]\n",
    "\n",
    "# Drop rows with 'any' or 'various' or 'depends on stock' in the 'model' column\n",
    "df['model'] = df['model'].fillna('')\n",
    "df = df[~df['model'].str.contains('any')]\n",
    "df = df[~df['model'].str.contains('various')]\n",
    "df = df[~df['model'].str.contains('depends on stock')]\n",
    "\n",
    "# Drop rows with '/' in the 'brand' or 'model' column\n",
    "df['brand'] = df['brand'].fillna('')\n",
    "df = df[~df['brand'].str.contains('/')]\n",
    "df = df[~df['model'].str.contains('/')]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove none working devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all entries which don't contain 'Good', 'Used', 'New', 'Excellent' or 'Refurbished' in the Condition column\n",
    "df = df[df['condition'].str.contains('good|used|new|excellent|refurbished')]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU generation splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Processor column into Processor i series and Processor generation columns\n",
    "df['processor i series'] = df['processor'].str.extract(r'(i\\d+)')\n",
    "df['processor generation'] = df['processor'].str.extract(r'(\\d+st|\\d+nd|\\d+rd|\\d+th|\\d+st gen|\\d+nd gen|\\d+rd gen|\\d+th gen)')\n",
    "\n",
    "# Reorder the columns\n",
    "cols = ['price', 'brand', 'processor i series', 'processor generation',\n",
    "        'processor speed', 'ram size', 'ssd capacity', 'storage type', 'screen size', 'graphics processing type', 'gpu', 'operating system', 'type', \n",
    "         'model','series', 'condition', 'processor', 'features', 'seller notes', 'title', 'link','sold_date', 'item number']\n",
    "df = df[cols]\n",
    "df = df.rename(columns={'ssd capacity': 'storage capacity'})\n",
    "\n",
    "# Drop rows where 'processor i series' or 'processor generation' contain NaN entries#\n",
    "df.dropna(subset=['processor i series', 'processor generation'], inplace=True)\n",
    "\n",
    "# Strip th from 'processor generation' column\n",
    "df['processor generation'] = df['processor generation'].str.replace(r'(st|rd|nd|th)', '')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove 'GHz' suffix from 'Processor Speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'GHz' suffix from CPU speed\n",
    "df['processor speed'] = df['processor speed'].str.replace('ghz', '')\n",
    "\n",
    "# Drop rows with NaN values in the 'processor speed' column\n",
    "df.dropna(subset=['processor speed'], inplace=True)\n",
    "\n",
    "# Drop rows with any non-float values in the 'processor speed' column\n",
    "df = df[pd.to_numeric(df['processor speed'], errors='coerce').astype(float).notnull()]\n",
    "\n",
    "# Drop rows where 'processor speed' is more than 10\n",
    "df = df[df['processor speed'].astype(float) < 10]\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop mutiple drive devices, for ease of beta test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing storage type values\n",
    "df.dropna(subset=['storage type'], inplace=True)\n",
    "\n",
    "\n",
    "# Drop entries which contain 'hdd + ssd' in the 'storage type' column\n",
    "df = df[~df['storage type'].str.contains('\\+')]\n",
    "df = df[~df['storage type'].str.contains('or')]\n",
    "df = df[~df['storage type'].str.contains('and')]\n",
    "\n",
    "# List unique entries in the 'storage type' column\n",
    "print(df['storage type'].unique())\n",
    "\n",
    "\n",
    "# Map storage types to categories\n",
    "storage_map = {'emmc': 'emmc', 'm.2 ssd': 'm.2', 'm.2 drive': 'm.2', 'nvme': 'nvme', 'ssd nvme': 'nvme', 'sshd (solid state hybrid drive)': 'sshd', 'sshd': 'sshd', 'ssd (solid state drive)': 'ssd', 'ssd': 'ssd', 'hdd (hard disk drive)': 'hdd', 'hdd': 'hdd'}\n",
    "df['storage type'] = df['storage type'].str.lower().apply(lambda x: next((v for k, v in storage_map.items() if k in x), x))\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Capacity TB to GB conversion, remove GB suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TB to GB\n",
    "df['storage capacity'] = df['storage capacity'].astype(str)\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('1tb', '1024gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('2tb', '2048gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('3tb', '3072gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('4tb', '4096gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('5tb', '5120gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('6tb', '6144gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('7tb', '7168gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('8tb', '8192gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('9tb', '9216gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('10tb', '10240gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('11tb', '11264gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('12tb', '12288gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('13tb', '13312gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('14tb', '14336gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('15tb', '15360gb')\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('16tb', '16384gb')\n",
    "\n",
    "\n",
    "# Remove GB from Storage Capacity\n",
    "df['storage capacity'] = df['storage capacity'].str.replace('gb', '')\n",
    "\n",
    "# Drop rows with non numerical values in the 'storage capacity' column\n",
    "df = df[pd.to_numeric(df['storage capacity'], errors='coerce').astype(float).notnull()]\n",
    "\n",
    "# Drop rows where 'storage capacity' contains a decimal point\n",
    "df = df[~df['storage capacity'].astype(str).str.contains('\\.')]\n",
    "\n",
    "# Drop rows where 'storage capacity' is less than 64\n",
    "df = df[df['storage capacity'].astype(int) >= 64]\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove GB suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove GB from RAM size\n",
    "df['ram size'] = df['ram size'].astype(str)\n",
    "df['ram size'] = df['ram size'].str.replace('gb', '')\n",
    "\n",
    "# If ram size = storage capacity, set ram size to 8gb\n",
    "df.loc[df['ram size'] == df['storage capacity'], 'ram size'] = '8'\n",
    "\n",
    "# Drop rows with non-numerical values in the 'ram size' column\n",
    "df = df[pd.to_numeric(df['ram size'], errors='coerce').astype(float).notnull()]\n",
    "\n",
    "# Drop rows where ram size is more than 64gb\n",
    "df = df[df['ram size'].astype(int) <= 64]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert screen size values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all non-numerical characters from the 'screen size' column\n",
    "import re\n",
    "\n",
    "df['screen size'] = df['screen size'].apply(lambda x: re.findall('\\d+\\.\\d+|\\d+', str(x))[0])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Column cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NaN entries in the 'gpu' column to 'integrated'\n",
    "df['gpu'] = df['gpu'].fillna('integrated')\n",
    "\n",
    "# Convert enties conatining 'intel' but no 'nvidia' or 'amd' to 'integrated'\n",
    "df.loc[df['gpu'].str.contains('intel|integrated|hd|uhd|onboard') & ~df['gpu'].str.contains('nvidia|amd'), 'gpu'] = 'integrated'\n",
    "\n",
    "# If entry contains '&' split string at '&' and keep the second part without the '&' symbol\n",
    "df.loc[df['gpu'].str.contains('&|\\+'), 'gpu'] = df['gpu'].str.split('&|\\+').str[1]\n",
    "\n",
    "# Drop 'graphics processing type' column\n",
    "df.drop(columns=['graphics processing type'], inplace=True)\n",
    "\n",
    "# If 'gpu' entry contains 'gtx' or 'radeon' but does not contain 'nvidia' or 'amd' respectively, add 'nvidia' or 'amd' to the entry\n",
    "df.loc[df['gpu'].str.contains('gtx') & ~df['gpu'].str.contains('nvidia'), 'gpu'] = 'nvidia ' + df['gpu']\n",
    "df.loc[df['gpu'].str.contains('radeon') & ~df['gpu'].str.contains('amd'), 'gpu'] = 'amd ' + df['gpu']\n",
    "\n",
    "# Strip whitespace from 'gpu' column\n",
    "df['gpu'] = df['gpu'].str.strip()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove entries with no sold date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no sold date\n",
    "df.dropna(subset=['sold_date'], inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('C:/Users/jack/Documents/temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model/Series column clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a Pandas DataFrame called 'df' with a 'model' column\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df2 = pd.read_csv('C:/Users/jack/Documents/temp.csv')\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('C:/Users/jack/Documents/EbayMarketScraping/Scraping_bs4_ebay-main/Master Analytics - Aiken Import Dump.csv')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n",
    "\n",
    "# Designate Product Type to filter\n",
    "filter = 'LAPTOP'\n",
    "\n",
    "# Drop rows that do not contain 'LAPTOP' in the 'Product Type' column\n",
    "df = df[df['ProductType'].str.contains(filter, na=False)]\n",
    "\n",
    "# Convert the 'Model' column to a string\n",
    "df['Model'] = df['Model'].astype(str)\n",
    "\n",
    "# Create a list of unique entries in the 'Model' column to compare against\n",
    "model_list = df['Model'].unique().tolist()\n",
    "\n",
    "# Lowercase all strings in the list\n",
    "model_list = [string.lower() for string in model_list]\n",
    "\n",
    "\n",
    "\n",
    "# If 'model' entry string in df2 is only numeric, append 'series' entry to start of the string\n",
    "for i, model in enumerate(df2['model']):\n",
    "    if str(model).isnumeric():\n",
    "        df2.at[i, 'model'] = str(df2.at[i, 'series']) + ' ' + model\n",
    "\n",
    "\n",
    "# Loop through each entry in the 'model' column and replace it with the most similar entry from the model_list\n",
    "for i, model in enumerate(df2['model']):\n",
    "    # Use the process.extractOne() method to find the most similar entry in the model_list\n",
    "    best_match = process.extractOne(str(model), model_list)\n",
    "    # If the similarity score is above a certain threshold, replace the original entry with the best match\n",
    "    if best_match[1] > 75:\n",
    "        df2.at[i, 'model'] = best_match[0]\n",
    "    if best_match[1] < 75:\n",
    "        df2.drop(i, inplace=True)    \n",
    "\n",
    "'''\n",
    "# If 'model' entry string in df2 contains corresponding 'brand' string in df2, remove brand string from model string\n",
    "for i, model in enumerate(df2['model']):\n",
    "    if df2.at[i, 'brand'] in str(model):\n",
    "        df2.at[i, 'model'] = model.strip(df2.at[i, 'brand'])\n",
    "'''\n",
    "\n",
    "# Strip 'notebook' and 'pc', 'laptop' and whitespace from the model entries\n",
    "df2['model'] = df2['model'].str.replace('notebook', '')\n",
    "df2['model'] = df2['model'].str.replace('pc', '')\n",
    "df2['model'] = df2['model'].str.replace('laptop', '')\n",
    "df2['model'] = df2['model'].str.strip()\n",
    "\n",
    "\n",
    "# Drop rows where model entry string = 'nan'\n",
    "df2.drop(df2[(df2['model'] == 'cheap gaming laptop') | (df2['model'] == 'nan')].index, inplace=True)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(df2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframe as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as a CSV file and display it\n",
    "df2.to_csv('C:/Users/jack/Documents/02-11-23_clean_output8719row.csv', encoding='utf-8', index=False)\n",
    "\n",
    "df = pd.read_csv('C:/Users/jack/Documents/02-11-23_clean_output8719row.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and list all unique entries in the 'model' column\n",
    "model_list = df['model'].unique().tolist()\n",
    "print(model_list)\n",
    "print(len(model_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Pandas DataFrame\n",
    "df_mod = pd.read_csv('E:\\EbayMarketScraping\\Scraping_bs4_ebay-main\\Master Analytics - Aiken Import Dump.csv')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df_mod)\n",
    "\n",
    "# Designate Product Type to filter\n",
    "filter = 'LAPTOP'\n",
    "\n",
    "# Drop rows that do not contain 'LAPTOP' in the 'Product Type' column\n",
    "df_mod = df_mod[df_mod['ProductType'].str.contains(filter, na=False)]\n",
    "\n",
    "# Convert the 'Model' column to a string\n",
    "df_mod['Model'] = df_mod['Model'].astype(str)\n",
    "\n",
    "# Create a list of unique entries in the 'Model' column\n",
    "model_list = df_mod['Model'].unique().tolist()\n",
    "\n",
    "# Add the 'Manufacturer' entry to the beginning of each string in the list\n",
    "manufacturer_model_list = [df_mod.loc[df_mod['Model'] == model, 'Manufacturer'].fillna('').iloc[0] + ' ' + model for model in model_list if 'APPLE' not in model]\n",
    "\n",
    "# Remove any string that contains the word 'APPLE'\n",
    "query_list = [string for string in manufacturer_model_list if 'APPLE' not in string]\n",
    "\n",
    "# Lowercase all strings in the list\n",
    "query_list = [string.lower() for string in query_list]\n",
    "\n",
    "# Remove 'notebook' and 'pc' and 'laptop' from the entries is the list\n",
    "query_list = [string.replace('notebook', '') for string in query_list]\n",
    "query_list = [string.replace('pc', '') for string in query_list]\n",
    "query_list = [string.replace('laptop', '') for string in query_list]\n",
    "\n",
    "# Strip whitespace from entries in the list\n",
    "query_list = [string.strip() for string in query_list]\n",
    "\n",
    "# Print the list of strings with the 'Manufacturer' entry added\n",
    "print(manufacturer_model_list)\n",
    "print(len(manufacturer_model_list))\n",
    "print(query_list)\n",
    "print(len(query_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
